{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "# template = PromptTemplate.from_template(\"{country_a}와 {country_b} 사이의 거리가 어떻게 돼?\")\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\",\n",
    "                  temperature = \"0.1\")\n",
    "\n",
    "# prompt = template.format(country_a = \"이탈리아\", country_b= \"한국\")\n",
    "\n",
    "# chat.predict(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = ChatPromptTemplate.from_messages([(\"system\", \"너는 대답을 할때 {max_items} 갯수만큼만 답해야하고, 콤마로 구분된 리스트로만 대답해야해.\"), (\"human\", \"{question}\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "\n",
    "    def parse(self, text):\n",
    "        items = text.strip().split(\",\")\n",
    "        return list(map(str.strip, items))\n",
    "    \n",
    "p = CommaOutputParser()\n",
    "\n",
    "p.parse(\"Hello, how, are, yous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "messages= [ HumanMessage(content = \"안녕, 네 이름은 뭐야? 오늘의 날씨는 어때?\"), AIMessage(content= \"안녕, 내 이름은 하이야.\"), SystemMessage(content=\"너는 친구야. 그래서 반말로 대답을 해.\")]\n",
    "chat.predict_messages(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = template | chat | CommaOutputParser()\n",
    "chain.invoke({\"max_items\" : 5, \"question\": \"포켓몬을 얘기해줘.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import ChatMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"country\": \"France\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Italy\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Greece\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"What do you know about {country}?\"),\n",
    "        (\"ai\", \"{answer}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a geography expert, you give short answers.\"),\n",
    "        example_prompt,\n",
    "        (\"human\", \"What do you know about {country}?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = final_prompt | chat\n",
    "\n",
    "chain.invoke({\"country\": \"Thailand\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "intro = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a role playing assistant.\n",
    "    And you are impersonating a {character}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "example = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    This is an example of how you talk:\n",
    "\n",
    "    Human: {example_question}\n",
    "    You: {example_answer}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "start = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Start now!\n",
    "\n",
    "    Human: {question}\n",
    "    You:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "final = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    {intro}\n",
    "                                     \n",
    "    {example}\n",
    "                              \n",
    "    {start}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    (\"intro\", intro),\n",
    "    (\"example\", example),\n",
    "    (\"start\", start),\n",
    "]\n",
    "\n",
    "\n",
    "full_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=final,\n",
    "    pipeline_prompts=prompts,\n",
    ")\n",
    "\n",
    "\n",
    "chain = full_prompt | chat\n",
    "\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"character\": \"Pirate\",\n",
    "        \"example_question\": \"What is your location?\",\n",
    "        \"example_answer\": \"Arrrrg! That is a secret!! Arg arg!!\",\n",
    "        \"question\": \"What is your fav food?\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.storage import LocalFileStore\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./files/chapterOne.pdf\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "vectorstore = Chroma.from_documents(docs, cached_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vectorstore.similarity_search(\"where does winston live\")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./files/chapterOne.pdf\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"refine\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    ")\n",
    "\n",
    "chain.run(\"Describe Victory Mansions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.txt\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "map_doc_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Use the following portion of a long document to see if any of the text is relevant to answer the question. Return any relevant text verbatim. If there is no relevant text, return : ''\n",
    "            -------\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "map_doc_chain = map_doc_prompt | llm\n",
    "\n",
    "\n",
    "def map_docs(inputs):\n",
    "    documents = inputs[\"documents\"]\n",
    "    question = inputs[\"question\"]\n",
    "    return \"\\n\\n\".join(\n",
    "        map_doc_chain.invoke(\n",
    "            {\"context\": doc.page_content, \"question\": question}\n",
    "        ).content\n",
    "        for doc in documents\n",
    "    )\n",
    "\n",
    "\n",
    "map_chain = {\n",
    "    \"documents\": retriever,\n",
    "    \"question\": RunnablePassthrough(),\n",
    "} | RunnableLambda(map_docs)\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Given the following extracted parts of a long document and a question, create a final answer. \n",
    "            If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "            ------\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = {\"context\": map_chain, \"question\": RunnablePassthrough()} | final_prompt | llm\n",
    "\n",
    "chain.invoke(\"How many ministries are mentioned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"[INST]What is the meaning of {word}[/INST]\")\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    model_kwargs={\n",
    "        \"max_new_tokens\": 250,\n",
    "    },\n",
    ")\n",
    "llm.client.api_url = 'https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1'\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({\"word\": \"potato\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "api_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")  # 여기에 실제 토큰을 입력하세요\n",
    "print(api_token)\n",
    "headers = {\"Authorization\": f\"Bearer {api_token}\"}\n",
    "\n",
    "response = requests.get(\"https://huggingface.co/api/whoami\", headers=headers)\n",
    "print(f\"Status code: {response.status_code}\")\n",
    "print(f\"Content: {response.json()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "\n",
    "def extract_audio_from_video(video_path, audio_path):\n",
    "    command = [\n",
    "        \"ffmpeg\",\n",
    "        \"-i\",\n",
    "        video_path,\n",
    "        \"-vn\",\n",
    "        audio_path,\n",
    "    ]\n",
    "    subprocess.run(command)\n",
    "\n",
    "\n",
    "extract_audio_from_video(\n",
    "    \"./files/test.mp4\",\n",
    "    \"./files/test.mp3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from pydub import AudioSegment\n",
    "\n",
    "\n",
    "def extract_audio_from_video(video_path, audio_path):\n",
    "    command = [\n",
    "        \"ffmpeg\",\n",
    "        \"-i\",\n",
    "        video_path,\n",
    "        \"-vn\",\n",
    "        audio_path,\n",
    "    ]\n",
    "    subprocess.run(command)\n",
    "\n",
    "\n",
    "track = AudioSegment.from_mp3(\"./files/test.mp3\")\n",
    "\n",
    "ten_minutes = 10 * 60 * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "chunks = math.ceil(len(track) / ten_minutes)\n",
    "\n",
    "for i in range(chunks):\n",
    "    start_time = i * ten_minutes\n",
    "    end_time = (i + 1) * ten_minutes\n",
    "\n",
    "    chunk = track[start_time:end_time]\n",
    "\n",
    "    chunk.export(f\"./files/chunks/chunk_{i}.mp3\", format=\"mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import final\n",
    "import openai\n",
    "import glob\n",
    "\n",
    "\n",
    "def transcribe_chunks(chunk_folder, destination):\n",
    "    files = glob.glob(f\"{chunk_folder}/*.mp3\")\n",
    "    final_transcript = \"\"\n",
    "    for file in files:\n",
    "        with open(file, \"rb\") as audio_file:\n",
    "            transcript = openai.Audio.transcribe(\n",
    "                \"whisper-1\",\n",
    "                audio_file,\n",
    "            )\n",
    "            final_transcript += transcript[\"text\"]\n",
    "    with open(destination, \"w\") as file:\n",
    "        file.write(final_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcribe_chunks(\"./files/chunks\", \"./files/transcript.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = openai.Audio.transcribe(\"whisper-1\", open(\"./files/chunks/chunk_0.mp3\", \"rb\"))\n",
    "\n",
    "transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import StructuredTool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "\n",
    "def plus(a, b):\n",
    "    return a + b\n",
    "\n",
    "\n",
    "agent = initialize_agent(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    tools=[\n",
    "        StructuredTool.from_function(\n",
    "            func=plus,\n",
    "            name=\"Sum Calculator\",\n",
    "            description=\"Use this to perform sums of two numbers. This tool take two arguments, both  should be numbers. Please finish the work.\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "prompt = \"Tell me Total Cost of $355.39 + $924.87 + $721.2 + $1940.29 + $573.63 + $65.72 + $35.00 + $552.00 + $76.16 + $29.12\"\n",
    "\n",
    "agent.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import StructuredTool, Tool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "\n",
    "def plus(inputs):\n",
    "    a, b = inputs.split(\",\")\n",
    "    return float(a) + float(b)\n",
    "\n",
    "\n",
    "agent = initialize_agent(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    handle_parsing_errors=True,\n",
    "    tools=[\n",
    "        Tool.from_function(\n",
    "            func=plus,\n",
    "            name=\"Sum Calculator\",\n",
    "            description=\"Use this to perform sums of two numbers. Use this tool by sending a pair of number separated by a comma.\\nExample:1,2\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "prompt = \"Cost of $355.39 + $924.87 + $721.2 + $1940.29 + $573.63 + $65.72 + $35.00 + $552.00 + $76.16 + $29.12\"\n",
    "\n",
    "agent.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Type\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import BaseTool\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.utilities import DuckDuckGoSearchAPIWrapper\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "\n",
    "class StockMarketSymbolSearchToolArgsSchema(BaseModel):\n",
    "    query: str = Field(\n",
    "        description=\"The query you will search for.Example query: Stock Market Symbol for Apple Company\"\n",
    "    )\n",
    "\n",
    "\n",
    "class StockMarketSymbolSearchTool(BaseTool):\n",
    "    name = \"StockMarketSymbolSearchTool\"\n",
    "    description = \"\"\"\n",
    "    Use this tool to find the stock market symbol for a company.\n",
    "    It takes a query as an argument.\n",
    "    \n",
    "    \"\"\"\n",
    "    args_schema: Type[\n",
    "        StockMarketSymbolSearchToolArgsSchema\n",
    "    ] = StockMarketSymbolSearchToolArgsSchema\n",
    "\n",
    "    def _run(self, query):\n",
    "        ddg = DuckDuckGoSearchAPIWrapper()\n",
    "        return ddg.run(query)\n",
    "\n",
    "\n",
    "agent = initialize_agent(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    agent=AgentType.OPENAI_FUNCTIONS,\n",
    "    handle_parsing_errors=True,\n",
    "    tools=[\n",
    "        StockMarketSymbolSearchTool(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "prompt = \"Give me information on Cloudflare's stock and help me analyze if it's a potential good investment. Also tell me what symbol does the stock have.\"\n",
    "\n",
    "agent.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import DuckDuckGoSearchResults\n",
    "\n",
    "search = DuckDuckGoSearchResults()\n",
    "search.run(\"Nico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from typing import Type\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import BaseTool\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.utilities import DuckDuckGoSearchAPIWrapper\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1, model_name=\"gpt-4o-mini\")\n",
    "\n",
    "alpha_vantage_api_key = os.environ.get(\"ALPHA_VANTAGE_API_KEY\")\n",
    "\n",
    "\n",
    "class StockMarketSymbolSearchToolArgsSchema(BaseModel):\n",
    "    query: str = Field(\n",
    "        description=\"The query you will search for.Example query: Stock Market Symbol for Apple Company\"\n",
    "    )\n",
    "\n",
    "\n",
    "class StockMarketSymbolSearchTool(BaseTool):\n",
    "    name = \"StockMarketSymbolSearchTool\"\n",
    "    description = \"\"\"\n",
    "    Use this tool to find the stock market symbol for a company.\n",
    "    It takes a query as an argument.\n",
    "    \n",
    "    \"\"\"\n",
    "    args_schema: Type[\n",
    "        StockMarketSymbolSearchToolArgsSchema\n",
    "    ] = StockMarketSymbolSearchToolArgsSchema\n",
    "\n",
    "    def _run(self, query):\n",
    "        ddg = DuckDuckGoSearchAPIWrapper()\n",
    "        return ddg.run(query)\n",
    "\n",
    "\n",
    "class CompanyOverviewArgsSchema(BaseModel):\n",
    "    symbol: str = Field(\n",
    "        description=\"Stock symbol of the company.Example: AAPL,TSLA\",\n",
    "    )\n",
    "\n",
    "\n",
    "class CompanyOverviewTool(BaseTool):\n",
    "    name = \"CompanyOverview\"\n",
    "    description = \"\"\"\n",
    "    Use this to get an overview of the financials of the company.\n",
    "    You should enter a stock symbol.\n",
    "    \"\"\"\n",
    "    args_schema: Type[CompanyOverviewArgsSchema] = CompanyOverviewArgsSchema\n",
    "\n",
    "    def _run(self, symbol):\n",
    "        r = requests.get(\n",
    "            f\"https://www.alphavantage.co/query?function=OVERVIEW&symbol={symbol}&apikey={alpha_vantage_api_key}\"\n",
    "        )\n",
    "        return r.json()\n",
    "\n",
    "\n",
    "class CompanyIncomeStatementTool(BaseTool):\n",
    "    name = \"CompanyIncomeStatement\"\n",
    "    description = \"\"\"\n",
    "    Use this to get the income statement of a company.\n",
    "    You should enter a stock symbol.\n",
    "    \"\"\"\n",
    "    args_schema: Type[CompanyOverviewArgsSchema] = CompanyOverviewArgsSchema\n",
    "\n",
    "    def _run(self, symbol):\n",
    "        r = requests.get(\n",
    "            f\"https://www.alphavantage.co/query?function=INCOME_STATEMENT&symbol={symbol}&apikey={alpha_vantage_api_key}\"\n",
    "        )\n",
    "        return r.json()[\"annualReports\"]\n",
    "\n",
    "\n",
    "class CompanyStockPerformanceTool(BaseTool):\n",
    "    name = \"CompanyStockPerformance\"\n",
    "    description = \"\"\"\n",
    "    Use this to get the weekly performance of a company stock.\n",
    "    You should enter a stock symbol.\n",
    "    \"\"\"\n",
    "    args_schema: Type[CompanyOverviewArgsSchema] = CompanyOverviewArgsSchema\n",
    "\n",
    "    def _run(self, symbol):\n",
    "        r = requests.get(\n",
    "            f\"https://www.alphavantage.co/query?function=TIME_SERIES_WEEKLY&symbol={symbol}&apikey={alpha_vantage_api_key}\"\n",
    "        )\n",
    "        return r.json()\n",
    "\n",
    "\n",
    "agent = initialize_agent(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    agent=AgentType.OPENAI_FUNCTIONS,\n",
    "    handle_parsing_errors=True,\n",
    "    tools=[\n",
    "        StockMarketSymbolSearchTool(),\n",
    "        CompanyOverviewTool(),\n",
    "        CompanyIncomeStatementTool(),\n",
    "        CompanyStockPerformanceTool(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "prompt = \"Give me financial information on Cloudflare's stock, considering its financials, income statements and stock performance help me analyze if it's a potential good investment.\"\n",
    "\n",
    "agent.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_sql_agent, AgentType\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain.sql_database import SQLDatabase\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    model_name=\"gpt-4-1106-preview\",\n",
    ")\n",
    "db = SQLDatabase.from_uri(\"sqlite:///movies.sqlite\")\n",
    "toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
    "\n",
    "agent = create_sql_agent(\n",
    "    llm=llm,\n",
    "    toolkit=toolkit,\n",
    "    agent_type=AgentType.OPENAI_FUNCTIONS,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "agent.invoke(\n",
    "    \"Give me the movies that have the highest votes but the lowest budgets and give me the name of their directors also include their gross revenue.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def load_memory(_):\n",
    "    Memory = memory.load_memory_variables({})[\"history\"]\n",
    "    print(Memory)\n",
    "    return Memory\n",
    "\n",
    "\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | llm\n",
    "\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\"question\": question})\n",
    "    memory.save_context(\n",
    "        {\"input\": question},\n",
    "        {\"output\": result.content},\n",
    "    )\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoke_chain(\"My name is nico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoke_chain(\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from langchain.document_loaders import AsyncChromiumLoader\n",
    "from langchain.document_transformers import Html2TextTransformer\n",
    "\n",
    "nest_asyncio.apply()\n",
    "html2text_transformer = Html2TextTransformer()\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "loader = AsyncChromiumLoader(\n",
    "    urls=[\"https://pubmed.ncbi.nlm.nih.gov/?term=MS&filter=simsearch2.ffrft\"],\n",
    "    headers=headers\n",
    ")\n",
    "docs = await loader.load()\n",
    "transformed = html2text_transformer.transform_documents(docs)\n",
    "\n",
    "transformed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
